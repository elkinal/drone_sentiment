---
title: "Drone Sentiment Analysis"
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

```{r}
#| label: libraries

# Basic
library(tidyverse)
library(scales)

# For cleaning / filtering text
library(stringr)
library(wordcloud)
library(tm)
library(janeaustenr)
library(textstem)

# For rendering
library(rmarkdown)
library(stringr)

```

```{r}
#| label: tidying-data

data1 <- read_csv("data/data1.csv")
 
# The number of total responses
pre_length = nrow(data1)

# Filtering and cleaning responses
response <- data1 |>
  select(Q6, Q8) |>
  rename(
    reason = Q6, # text
    opinion = Q8 # number
  )|>
  mutate(reason = gsub("(\n|<br />)"," ",reason)) |> 
  mutate(reason = gsub("'","",reason)) |>
  mutate(reason = gsub("’","",reason)) |>
  drop_na() 

# Removing first 2 rows
response = response[-c(1, 2),]

# Reformatting opinion column
response$opinion = as.numeric(response$opinion)

# Lemmatizing words - reducing them to base form
lemmatize <- function(sentence) {
  return(paste(lemmatize_words(strsplit(sentence, " ")[[1]]),
        collapse=" "))
}
response[c("reason")] <- apply(response[c("reason")], 1,lemmatize)


# Function to turn a column into a corpus
create_corpus <- function(column) {
  
  # Creating Corpus for ALL RAW REASONS
  corpus <- VCorpus(
    VectorSource(
      as.vector(column))
    )
  
  corpus <- corpus |>
    tm_map(removeNumbers) |>
    tm_map(removePunctuation) |>
    tm_map(stripWhitespace) 
    
  corpus <- tm_map(corpus, content_transformer(tolower)) 
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  return (corpus)
}

# Corpus for all REASONS 
q6_corpus = create_corpus(response$reason)


# Creating Corpus for OPINIONS & REASONS
reason_list <- split(response, response$opinion)
opinion_corpus_list <- list()


# Corpuses for the REASONS, split per OPINION (1-5)
opinion_1_corpus = create_corpus(reason_list[[1]]$reason)
opinion_2_corpus = create_corpus(reason_list[[2]]$reason)
opinion_3_corpus = create_corpus(reason_list[[3]]$reason)
opinion_4_corpus = create_corpus(reason_list[[4]]$reason)
opinion_5_corpus = create_corpus(reason_list[[5]]$reason)

# Corpuses for the REASONS, split by GENERAL OPINION 
# where scores of 1 or 2 = oppose & 4 or 5 = support


# Creating corpuses for general support / opposition
opinion_support_corpus = create_corpus(
  rbind(reason_list[[1]], reason_list[[2]])$reason
)

opinion_oppose_corpus = create_corpus(
  rbind(reason_list[[4]], reason_list[[5]])$reason
)

# Calculating the number of real responses
post_length = nrow(response)
yield = round(((post_length / pre_length) * 100), 3)


```

The data has `r pre_length` responses. The percentage of non-blank responses is `r yield` %

```{r}
#| label: word-ratio

# Returns a word-frequency matrix from a corpus
get_wfm <- function(corpus) {
  dtm <- TermDocumentMatrix(corpus) 
  matrix <- as.matrix(dtm) 
  words <- sort(rowSums(matrix),decreasing=TRUE) 
  wfm <- data.frame(word = names(words),freq=words)
  
  return (wfm)
}


# Draws the word-frequency graph between 2 corpuses
draw_wfm_diff_graph <- function(corpus_1, corpus_2) {
  
  # Creating word-frequency matrix
  opinion_1_wfm = get_wfm(corpus_1)
  opinion_5_wfm = get_wfm(corpus_2)
  
  # Adding negative sign to opposing views
  opinion_1_wfm <- opinion_1_wfm |>
    mutate(freq = -freq)
  
  # Calculating the relative frequencies
  opinion_1_wfm <- opinion_1_wfm |>
    mutate(ratio = freq / (nrow(opinion_1_wfm)))
  
  opinion_5_wfm <- opinion_5_wfm |>
    mutate(ratio = freq / (nrow(opinion_5_wfm)))
  
  # Finding the difference in word frequencies
  opinion_diff <- rbind(opinion_1_wfm, opinion_5_wfm)
  opinion_diff <- opinion_diff |>
    group_by(word) |>
    summarize(diff_freq = sum(ratio)) |>
    arrange(desc(diff_freq))
  
  # Joining the most significant words
  largest_diff <- rbind(head(opinion_diff, 15), tail(opinion_diff, 15))
  
  
  # Drawing the grap
  largest_diff |>
    ggplot(
      aes(reorder(word, -diff_freq, sum), 
          diff_freq, fill = diff_freq < 0)) + 
    
    geom_bar(stat="identity") +
    
    coord_flip() +  
    ylab("Difference in word frequency") + 
    xlab("Word") +
    ggtitle("Word frequency of drone export supporters / opposers") + 
    
    scale_fill_manual(
      values=c("#77dd76", "#f69185"),
      name="Drone export\nopinion",
                           breaks=c("FALSE", "TRUE"),
                           labels=c("Support", "Oppose")) +
    
    
    ylim(-0.1,0.09) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 2)) 
}

draw_wfm_diff_graph(opinion_support_corpus, opinion_oppose_corpus)

# WORDS represent the difference in the ratio of words that have been used.
# for example, almost 0.04 , 4% of words used by suppoters is "good"


```

The difference in word frequency represents the percentage of

```{r}
#| label: generate-wordcloud

# Draws a Word Cloud based from a corpus
generate_wordcloud <- function(corpus) {
  wfm <- get_wfm(corpus)
  wfm <- wfm[c(-1, -2),] # removing the words "drone" and "country"
  set.seed(1234) 
  wordcloud(words = wfm$word, 
            freq = wfm$freq, 
            min.freq = 1,           
            max.words=200, 
            random.order=FALSE, 
            rot.per=0.35,            
            colors=brewer.pal(8, "Dark2"))
}
```

```{r}
#| label: strong-support-wordcloud

# Drawing worldcloud for STRONG OPPOSITION (1)
generate_wordcloud(opinion_1_corpus)
```

```{r}
#| label: strong-opposition-worldcloud

# Drawing worldcloud for STRONG SUPPORT (5)
generate_wordcloud(opinion_5_corpus)
```

     - Fourth, write two paragraphs, one on the sentiment analysis method and one on the key findings, integrating results by support, positive and negative sentiment, and by country. 
