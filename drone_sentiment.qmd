---
title: "Drone Sentiment Analysis"
author: "Alexey Elkin"
date: "06/15/2023"
format: pdf
editor: visual
fig-width: 6
fig-height: 5
---

# What do people think about drone exports?

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r}
#| label: libraries

# Basic
library(tidyverse)
library(scales)
library(knitr)

# For cleaning / filtering text
library(stringr)
library(wordcloud)
library(tm)
library(janeaustenr)
library(textstem)
library(syuzhet)
library(sentimentr)

# For rendering
library(rmarkdown)
library(stringr)
library(gridExtra)
library(grid)

```

```{r}
#| label: tidying-data

data1 <- read_csv("data/data1.csv")
 
# The number of total responses
pre_length = nrow(data1)

# Filtering responses
response <- data1 |>
  select(Q6, Q8) |>
  rename(
    reason = Q6, # text
    opinion = Q8 # number
  )

# Adding row indices
response$index <- 1:nrow(response)

# Response without filtering
pure_response <- response |>
  drop_na()

# Cleaning responses
response <- response |>
  
  # Removing punctuation
  mutate(reason = gsub("(\n|<br />)"," ",reason)) |> 
  mutate(reason = gsub("'","",reason)) |>
  mutate(reason = gsub("â€™","",reason)) |>
  
  # Prevent sentence splitting
  mutate(reason = gsub(".","",reason, fixed = TRUE)) |> 
  mutate(reason = gsub("?","",reason, fixed = TRUE)) |> 
  mutate(reason = gsub("!","",reason, fixed = TRUE)) |> 
  drop_na() 

# Removing first 2 rows
response = response[-c(1, 2),]

# Reformatting opinion column
response$opinion = as.numeric(response$opinion)

# Lemmatizing words - reducing them to base form
lemmatize <- function(sentence) {
  return(paste(lemmatize_words(strsplit(sentence, " ")[[1]]),
        collapse=" "))
}
response[c("reason")] <- apply(response[c("reason")], 1,lemmatize)

# Function to turn a column into a corpus
create_corpus <- function(column) {
  
  # Creating Corpus for ALL RAW REASONS
  corpus <- VCorpus(
    VectorSource(
      as.vector(column))
    )
  
  corpus <- corpus |>
    tm_map(removeNumbers) |>
    tm_map(removePunctuation) |>
    tm_map(stripWhitespace) 
    
  corpus <- tm_map(corpus, content_transformer(tolower)) 
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  return (corpus)
}

# Corpus for all REASONS 
q6_corpus = create_corpus(response$reason)


# Creating Corpus for OPINIONS & REASONS
reason_list <- split(response, response$opinion)
opinion_corpus_list <- list()


# Corpuses for the REASONS, split per OPINION (1-5)
opinion_1_corpus = create_corpus(reason_list[[1]]$reason)
opinion_2_corpus = create_corpus(reason_list[[2]]$reason)
opinion_3_corpus = create_corpus(reason_list[[3]]$reason)
opinion_4_corpus = create_corpus(reason_list[[4]]$reason)
opinion_5_corpus = create_corpus(reason_list[[5]]$reason)

# Corpuses for the REASONS, split by GENERAL OPINION 
# where scores of 1 or 2 = oppose & 4 or 5 = support

# Creating corpuses for general support / opposition
opinion_support_corpus = create_corpus(
  rbind(reason_list[[1]], reason_list[[2]])$reason
)

opinion_oppose_corpus = create_corpus(
  rbind(reason_list[[4]], reason_list[[5]])$reason
)

# Calculating the number of real responses
post_length = nrow(response)
yield = round(((post_length / pre_length) * 100), 3)

```

First, we clean and format the survey data to make it easier to analyse. All punctuation and special characters are removed, and all the words are *lemmatized* - reduced to their base form. For example, "walked", "walking" and "walks" will be reduced to "walk". This ensures that the words frequency tables and sentiment analysis doesn't have doubled words.

Then, the data is split along the person's answer to [Question 8]{.underline}: *"To what extent do you agree that U.S. officials have a moral obligation to sell U.S.-manufactured drones to allies and partners?"*. The responses are split into 5 groups based on their response to on a 1-5 scale, where users that selected [\[1\] Strongly Oppose]{.underline} drone exports and those who selected [\[5\] Strongly Support]{.underline} them.

Overall, data has `r pre_length` responses. The percentage of non-blank responses is `r yield` %

```{r}
#| label: word-ratio

# Returns a word-frequency matrix from a corpus
get_wfm <- function(corpus) {
  dtm <- TermDocumentMatrix(corpus) 
  matrix <- as.matrix(dtm) 
  words <- sort(rowSums(matrix),decreasing=TRUE) 
  wfm <- data.frame(word = names(words),freq=words)
  
  return (wfm)
}

# Draws the word-frequency graph between 2 corpuses
draw_wfm_diff_graph <- function(corpus_1, corpus_2, n_width, p_width) {
  
  # Creating word-frequency matrix
  opinion_1_wfm = get_wfm(corpus_1)
  opinion_5_wfm = get_wfm(corpus_2)
  
  # Adding negative sign to opposing views
  opinion_1_wfm <- opinion_1_wfm |>
    mutate(freq = -freq)
  
  # Calculating the relative frequencies
  opinion_1_wfm <- opinion_1_wfm |>
    mutate(ratio = freq / (nrow(opinion_1_wfm)))
  
  opinion_5_wfm <- opinion_5_wfm |>
    mutate(ratio = freq / (nrow(opinion_5_wfm)))
  
  # Finding the difference in word frequencies
  opinion_diff <- rbind(opinion_1_wfm, opinion_5_wfm)
  opinion_diff <- opinion_diff |>
    group_by(word) |>
    summarize(diff_freq = sum(ratio)) |>
    arrange(desc(diff_freq))
  
  # Joining the most significant words
  largest_diff <- rbind(head(opinion_diff, p_width), tail(opinion_diff, n_width))
  
  # Drawing the grap
  largest_diff |>
    ggplot(
      aes(reorder(word, -diff_freq, sum), 
          diff_freq, fill = diff_freq < 0)) + 
    
    geom_bar(stat="identity") +
    
    coord_flip() +  
    ylab("Difference in word frequency") + 
    xlab("Word") +
    ggtitle("Word frequency of drone export supporters / opposers") + 
    
    scale_fill_manual(
      values=c("#77dd76", "#f69185"),
      name="Drone export\nopinion",
                           breaks=c("FALSE", "TRUE"),
                           labels=c("Support", "Oppose")) +
    
    ylim(-0.1,0.09) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 2)) 
}

draw_wfm_diff_graph(opinion_support_corpus, opinion_oppose_corpus, 15, 15)


```

Then, we create a word frequency graph that covers all of the responses to [Question 6:]{.underline} *"What factors did you consider while evaluating your support for the export of U.S.-manufactured drones?".* Then, the respondents are split into 2 groups, those who support drone exports \[[Question 8:]{.underline} 4 or 5\], and those who oppose them \[[Question 8:]{.underline} 1 or 2\]. We calculate the ratio of each word to the total response length in their category. For example, "good" might make up 10% of words in the positive responses. After doing this for both categories, we find the difference between them, and plot a graph of a results.

The difference in word frequency represents the percentage of the difference between the words used. For example, "good" is featured 8% more frequently in supportive votes than in opposing ones. On the other hand, "war" and "weapon" are featured more frequently in opposing votes.

The reason the word frequencies for supporters peak much more than for people opposing drone exports is likely because opposing viewpoints tended to use a greater variety of language, whereas drone export supporters mostly used the same words like "good", "country" and "ally".

```{r}
#| label: oppose-frequency-detail

draw_wfm_diff_graph(opinion_support_corpus, opinion_oppose_corpus, 30, 0)
```

A closer analysis at the comments of the people opposing drone exports shows many of the factors people are concerned about. For example, people are concerned about the "humanitarian" aspects of drone exports, as well as mentioning "strikes" and "spy".

```{r}
#| label: support-frequency-detail

draw_wfm_diff_graph(opinion_support_corpus, opinion_oppose_corpus, 0, 30)
```

Similarly, looking at the comments by the people supporting drone exports reveals the reasoning behind their opinions. People mention "relationships", "economy", "sell", "help", and "support" very frequently.

```{r}
#| label: generate-wordcloud

# Draws a Word Cloud based from a corpus
generate_wordcloud <- function(corpus) {
  wfm <- get_wfm(corpus)
  wfm <- wfm[c(-1, -2),] # removing the words "drone" and "country"
  set.seed(1234) 
  wordcloud(words = wfm$word, 
            freq = wfm$freq, 
            min.freq = 1,           
            max.words=200, 
            random.order=FALSE, 
            rot.per=0.35,            
            colors=brewer.pal(8, "Dark2"))
}
```

```{r}
#| label: strong-support-wordcloud

# Drawing worldcloud for STRONG OPPOSITION (1)
generate_wordcloud(opinion_1_corpus)
```

A word cloud of words used by people who voted to [\[1\] Strongly Oppose]{.underline} drone exports.

```{r}
#| label: strong-opposition-worldcloud

# Drawing worldcloud for STRONG SUPPORT (5)
generate_wordcloud(opinion_5_corpus)
```

A word cloud of words used by people who voted to [\[5\] Strongly Support]{.underline} drone exports.

## Sentiment Analysis of Responses

To [Question 6]{.underline}: *"What factors did you consider while evaluating your support for the export of U.S.-manufactured drones?"*

#### 10 Most positive responses

```{r}
#| label: positive-responses

score_responses <- response |>
  mutate(score = get_sentiment(reason)) |>
  arrange(score) 


head(score_responses, 10) |>
  select(reason, score)
```

#### 10 Most Negative responses

```{r}
#| label: negative-responses

tail(score_responses, 10) |>
  select(reason, score)
```

We performed a sentiment analysis of the responses to [Question 6,]{.underline} and assigned "scores" to reach responses that measure how positive or negative they are. Then, we selected the 10 most extreme responses in each category, showing the most positive and negative arguments for drone exports to foreign countries.

```{r}
#| label: line-breaking

# Adds line breaks to text
prnt.test <- function(x){
   cat(x, sep="\n\n")
}

# Adds comma flags to text
break_words <- function(startstring) {
  words = strsplit(startstring, ' ')[[1L]]
  splits = cut(seq_along(words), breaks = seq(0L, length(words) + 10L, by = 10L))
  paste(lapply(split(words, splits), paste, collapse = ' '), collapse = '\n')
}

# Prints everything with line breaks every 10 words.
prnt_all_multi <- function(neg_lst) {
  for (x in 1:nrow(neg_lst)) {
    print(sprintf("[Response %s]", x))
    writeLines(break_words(neg_lst$reason[[x]]))  
    writeLines("\n")
  }
}

```

```{r}
#| label: pure-text

# Returns the 'pure' version of the text.
# This is the version without the punctuation filtering
# And without the words being reduced to their base form.

get_pure <- function(x) {
  
  df <- merge(x, pure_response, by="index", all=TRUE) |>
    drop_na(reason.x, score) |>
    select(reason.y)
  
  return (df)
}

hd <- head(score_responses, 100) |>
  select(reason, score, index)


```

The full text for the 10 most **positive** responses is below.

```{r}
#| label: detailed-positive

prnt_all_multi(
  get_pure(
    tail(score_responses, 10) |>
      select(reason, score, index)
    )
  )
```

Similarly, the full text for the10 most **negative** responses are here:

```{r}
#| label: detailed-negative

prnt_all_multi(
  get_pure(
    head(score_responses, 10) |>
      select(reason, score, index)
    )
  )
```
